{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from math import sqrt\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Conv2DTranspose, Lambda, Reshape, Flatten, Dense, BatchNormalization\n",
    "from tensorflow.keras.losses import KLDivergence\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "import tensorflow.python.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset preprocessing\n",
    "# Only needs to be run once, Tyler ran it already\n",
    "\n",
    "# DATA_DIR = \"/hdd/datasets/UTKFace\"\n",
    "# image_paths = glob(DATA_DIR + \"/*\")\n",
    "\n",
    "# images = np.zeros((len(image_paths), 96, 96, 3), dtype=np.float32)\n",
    "# for i, image_path in enumerate(image_paths):\n",
    "#     image = Image.open(image_path)\n",
    "#     image = image.resize((96, 96), Image.LANCZOS)\n",
    "#     images[i] = np.array(image) / 255.\n",
    "# np.save(\"/hdd/datasets/UTKFace.npy\", images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads numpy file containing dataset\n",
    "images = np.load(\"/hdd/datasets/UTKFace.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots some sample images\n",
    "fig, ax = plt.subplots(ncols=5, figsize=(15, 15))\n",
    "for i, col in enumerate(ax):\n",
    "    col.imshow(images[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the encoder using the tf.Keras functional API\n",
    "# Four of Conv2D->Conv2D->BatchNorm->MaxPool starting at 16 filters and doubling each time\n",
    "# Then a final, pixel-wise Conv2D with 128 filters\n",
    "def create_encoder(input_shape, encoding_dim):\n",
    "    if not sqrt(encoding_dim).is_integer():\n",
    "        raise ValueError(\"Encoding dim must be a perfect square.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the decoder using the tf.Keras functional API\n",
    "# Four of Conv2DTranspose starting at 128 filters and halving each time\n",
    "# Then a final Conv2D with 3 filters and sigmoid activation\n",
    "def create_decoder(encoding_dim):\n",
    "    if not sqrt(encoding_dim).is_integer():\n",
    "        raise ValueError(\"Encoding dim must be a perfect square.\")\n",
    "        \n",
    "    inputs = Input(shape=(int(sqrt(encoding_dim)), int(sqrt(encoding_dim)), 128))\n",
    "    conv1 = Conv2DTranspose(128, 3, strides=2, padding=\"same\", activation=\"relu\")(inputs)\n",
    "    conv2 = Conv2DTranspose(64, 3, strides=2, padding=\"same\", activation=\"relu\")(conv1)\n",
    "    conv3 = Conv2DTranspose(32, 3, strides=2, padding=\"same\", activation=\"relu\")(conv2)\n",
    "    conv4 = Conv2DTranspose(16, 3, strides=2, padding=\"same\", activation=\"relu\")(conv3)\n",
    "    conv5 = Conv2D(3, 3, padding=\"same\", activation=\"sigmoid\")(conv4)\n",
    "    \n",
    "    decoder = Model(inputs, conv5)\n",
    "    decoder.summary()\n",
    "    \n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = images[0].shape\n",
    "ENCODING_DIM = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assemble your autoencoder and compile with MSE loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p models/ae\n",
    "checkpointer = ModelCheckpoint(\"models/ae/epoch{epoch}_loss{val_loss:.4f}.h5\", save_best_only=True, verbose=1)\n",
    "\n",
    "# Fit the model for 50 epochs with the checkpointer callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads best model\n",
    "saved_models = sorted(glob(\"models/ae/*\"), key=os.path.getmtime)\n",
    "autoencoder = load_model(saved_models[-1])\n",
    "encoder = autoencoder.get_layer(index=1)\n",
    "decoder = autoencoder.get_layer(index=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots some example reconstructions\n",
    "\n",
    "visual = images[:5]\n",
    "preds = autoencoder.predict(visual)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range(5):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(visual[i])\n",
    "    plt.xlabel(\"Original\")\n",
    "for i in range(5):\n",
    "    plt.subplot(5,5,i+6)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(preds[i])\n",
    "    plt.xlabel(\"Reconstructed\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compares the mean of the reconstructions with the reconstruction of the mean latent value\n",
    "# Just thought this was cool :)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "plt.subplot(5, 5, 1)\n",
    "preds = autoencoder.predict(images)\n",
    "avg = np.mean(images, axis=0)\n",
    "plt.imshow(avg)\n",
    "\n",
    "plt.subplot(5, 5, 2)\n",
    "latent = encoder.predict(images)\n",
    "avg = np.expand_dims(np.mean(latent, axis=0), 0)\n",
    "pred = decoder.predict(avg).squeeze()\n",
    "plt.imshow(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the variational encoder\n",
    "# Same as the earlier encoder, but with tfp.layers.Convolution2DFlipout at the end\n",
    "# This layer learns a distribution over each weight\n",
    "def create_variational_encoder(input_shape, encoding_dim):\n",
    "    if not sqrt(encoding_dim).is_integer():\n",
    "        raise ValueError(\"Encoding dim must be a perfect square.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the variational encoder\n",
    "# Same as the earlier decoder!\n",
    "def create_variational_decoder(encoding_dim):\n",
    "    if not sqrt(encoding_dim).is_integer():\n",
    "        raise ValueError(\"Encoding dim must be a perfect square.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the ELBO loss, we need to create a \"closure\"\n",
    "# This is when we define a function within a function\n",
    "# We need this to include other variables in the KL term, but\n",
    "# Keras expects a certain function signature for the loss fn.\n",
    "def create_loss_fn(model, batch_size, dataset_size):\n",
    "    # Obtain the model KL divergence per epoch by sum(model.losses)\n",
    "    # Then re-weight it by batch / dataset\n",
    "\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        # Compute BCE and add KL\n",
    "    \n",
    "    return loss_fn\n",
    "\n",
    "# Compile the model with your custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models/vae\n",
    "checkpointer = ModelCheckpoint(\"models/vae/epoch{epoch}_loss{val_loss:.0f}.h5\", save_best_only=True, verbose=1)\n",
    "\n",
    "# Fit the model for 150 epochs with the checkpointer callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads best model\n",
    "saved_models = sorted(glob(\"models/vae/*\"), key=os.path.getmtime)\n",
    "variational_autoencoder = load_model(saved_models[-1])\n",
    "variational_encoder = autoencoder.get_layer(index=1)\n",
    "variational_decoder = autoencoder.get_layer(index=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots some example reconstructions\n",
    "# Why is this worse than the original autoencoder?\n",
    "# (Recall what adding KL divergence does!)\n",
    "visual = images[:5]\n",
    "\n",
    "# Runs 25 Monte Carlo samples\n",
    "# Necessary because model forward pass is stochastic\n",
    "preds = np.zeros((25, 5, 96, 96, 3))\n",
    "for i in range(0, 25):\n",
    "    preds[i] = variational_autoencoder.predict(visual)\n",
    "preds = preds.mean(axis=0)\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range(5):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(visual[i])\n",
    "    plt.xlabel(\"Original\")\n",
    "for i in range(5):\n",
    "    plt.subplot(5,5,i+6)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(preds[i])\n",
    "    plt.xlabel(\"Reconstructed\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compares the mean of the reconstructions with the reconstruction of the mean latent value\n",
    "# Why do you think this is different than the original autoencoder?\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "plt.subplot(5, 5, 1)\n",
    "preds = variational_autoencoder.predict(images)\n",
    "avg = np.mean(images, axis=0)\n",
    "plt.imshow(avg)\n",
    "\n",
    "plt.subplot(5, 5, 2)\n",
    "latent = variational_encoder.predict(images)\n",
    "avg = np.expand_dims(np.mean(latent, axis=0), 0)\n",
    "pred = variational_decoder.predict(avg).squeeze()\n",
    "plt.imshow(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to play around with architectures and encoding_dim values!\n",
    "# Post any cool results on the Slack.\n",
    "\n",
    "\n",
    "# CHALLENGE PROBLEM:\n",
    "# I wasn't able to figure out a way to plot the latent space of the VAE \n",
    "# to obtain an image like the MNIST plot in the slides\n",
    "# If you can figure out how to do this and display latent space plots\n",
    "# for both models, you'll win a prize!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
